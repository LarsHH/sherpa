{
    "docs": [
        {
            "location": "/",
            "text": "Hobbit\n\n\nWelcome to Hobbit aka. Hyperparameter Optimization By BandITs. Hobbit's goal is to give you a tool for fast and easy\nhyperparameter search.\n\n\nMotivation\n\n\nThe main idea behind Hobbit is to make an extremely easy to use, Keras compatible, framework for hyperparameter\noptimization. It works by specifying a model template with modifiable hyperparameters, then it automatically trains as\nmany models as wanted and as long as needed using ranges specified by the user for the tuneable hyperparameters.",
            "title": "Home"
        },
        {
            "location": "/#hobbit",
            "text": "Welcome to Hobbit aka. Hyperparameter Optimization By BandITs. Hobbit's goal is to give you a tool for fast and easy\nhyperparameter search.",
            "title": "Hobbit"
        },
        {
            "location": "/#motivation",
            "text": "The main idea behind Hobbit is to make an extremely easy to use, Keras compatible, framework for hyperparameter\noptimization. It works by specifying a model template with modifiable hyperparameters, then it automatically trains as\nmany models as wanted and as long as needed using ranges specified by the user for the tuneable hyperparameters.",
            "title": "Motivation"
        },
        {
            "location": "/first_steps/",
            "text": "First Steps\n\n\nIn this tutorial we will write the code to train a shallow neural network to predict handwritten digits from the MNIST dataset using Keras and optimize it's hyperparameters using Hobbit.\n\n\nThe Hyperparameters\n\n\nFirst we need to define distributions and ranges for the hyperparameters we want to include in our model training. To do this we define a list of Hyperparameter objects, each one of these will be tuned afterwards. For each Hyperparameter we must assign a \nname\n, a tuple with the range of values to explore (\nargs\n) and a \ndistribution\n which indicates how to sample values within the defined range.\n\n\nIn this example we want to optimize three hyperparameters: the dropout rate, the activation function and the learning rate. We use uniform(0, 1) for the dropout rate, a discrete distribution over \n('sigmoid', 'tanh', 'relu')\n for the activation and a log-uniform for the learning rate. The log-uniform distribution samples uniformly on the log scale and then transforms back. You can see more information about the available distributions \nhere\n.\n\n\nfrom hobbit import Hyperparameter\nmy_hparam_ranges = [Hyperparameter(name='learning_rate', distribution='log-uniform', distr_args=(0.0001, 0.1)),\n                    Hyperparameter(name='activation', distribution='choice', distr_args=[('sigmoid', 'tanh', 'relu')]),\n                    Hyperparameter(name='dropout', distribution='uniform', distr_args=(0., 1.))]\n\n\n\n\nThe model\n\n\nWith the principle of keeping Hobbit flexible we completely define the model we want to train inside a function which we call \nmy_model()\n. This function takes a \ndictionary of hyperparameters\n \nhparams\n as its only argument and returns a \ncompiled Keras model\n. Every model we train will follow this template and they will differ only in the values of the hyperparameters and how long they were trained.\n\n\nEvery optimizable hyperparameter within \nmy_model()\n corresponds to one of the values defined before as a Hyperparameter object. Here we will specify where each hyperparameter goes and we will access it from the \nhparams\n dictionary like this \n\nhparams['name_of_hyperparam']\n\n\nimport keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import RMSprop\n\ndef my_model(hparams):\n    model = Sequential()\n    model.add(Dropout(rate=hparams['dropout'], input_shape=(784,)))\n    model.add(Dense(100, activation=hparams['activation']))\n    model.add(Dropout(rate=hparams['dropout']))\n    model.add(Dense(10, activation='softmax'))\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=RMSprop(lr=hparams['learning_rate']),\n                  metrics=['accuracy'])\n    return model\n\n\n\n\nThe dataset\n\n\nThe dataset is loaded as a tuple of training and validation objects. These objects are usually numpy arrays with each sample as a row and each column as a feature. If there is some preprocessing for the data it has to be done before passing it to Hobbit. Here we load the MNIST dataset using Keras and do some simple preprocessing.\n\n\nnum_classes = 10\n\n# the data, shuffled and split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nx_train = x_train.reshape(60000, 784)\nx_test = x_test.reshape(10000, 784)\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.np_utils.to_categorical(y_test, num_classes)\n\nmy_dataset = (x_train, y_train), (x_test, y_test)\n\n\n\n\nHyperband\n\n\nWe have all the ingredients. Now we just need to create the algorithm. As an argument you need to specify a repository directory. This is where Hobbit will store all of your models and the table with the results.\n\n\nfrom hobbit.algorithms import Hyperband\nhband = Hyperband(model_function=my_model,\n                  dataset=my_dataset,\n                  hparam_ranges=my_hparam_ranges,\n                  repo_dir='./my_test_repo')\n\n\n\n\nRunning\n\n\nFinally, let's run the complete pipeline. Hyperband has two parameters:\n\n \nR\n: The budget of epochs per stage\n\n \neta\n: The cut-factor after each stage and which is also the factor by which training gets longer at every stage. For eta the theory-default is 3\n\n\nTo give you a feel for these here is an example for R=20 and eta=3. Here n_i is the number of configurations and r_i the number of epochs they are trained for. Hyperband makes multiple runs in which it does successive halving. smax corresponds to the current run.\n\n\nR = 20\neta = 3\n\nsmax=2\nn_0=9   r_0=2.22=2\nn_1=3   r_1=6.66=7\nn_2=1   r_2=20\n\nsmax=1\nn_0=5   r_0=6.66=7\nn_1=1   r_1=20\n\nsmax=0\nn_0=3   r_0=20\n\n\n\nNow let's run our own Hyperband. If you're on a CPU this may take a few minutes.\n\n\ntab = hband.run(R=20, eta=3)\n\n\n\n\nResults\n\n\nTo access the results for all configurations and their respective test error you can either use the returned Pandas dataframe from the \nrun()\n function or look at the CSV in your repository directory. Val Loss indicates the lowest seen validation loss for the configuration. The \nRun\n corresponds to the specific run it was trained at according to the Hyperband algorithm with run zero being the last and longest one. The \nID\n is an identifier of the model within the run.\n\n\ntab\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nHparams\n\n      \nID\n\n      \nRun\n\n      \nVal Loss\n\n    \n\n  \n\n  \n\n    \n\n      \n2-0\n\n      \n{'learning_rate': 0.0013383297653888713, 'acti...\n\n      \n0\n\n      \n2\n\n      \n0.079573\n\n    \n\n    \n\n      \n2-1\n\n      \n{'learning_rate': 0.00012199066883461425, 'act...\n\n      \n1\n\n      \n2\n\n      \n0.504462\n\n    \n\n    \n\n      \n2-2\n\n      \n{'learning_rate': 0.00012022855306991257, 'act...\n\n      \n2\n\n      \n2\n\n      \n0.361248\n\n    \n\n    \n\n      \n2-3\n\n      \n{'learning_rate': 0.01711893606634988, 'activa...\n\n      \n3\n\n      \n2\n\n      \n0.110820\n\n    \n\n    \n\n      \n2-4\n\n      \n{'learning_rate': 0.0002600277942901727, 'acti...\n\n      \n4\n\n      \n2\n\n      \n0.442517\n\n    \n\n    \n\n      \n2-5\n\n      \n{'learning_rate': 0.0004934370333196648, 'acti...\n\n      \n5\n\n      \n2\n\n      \n0.287478\n\n    \n\n    \n\n      \n2-6\n\n      \n{'learning_rate': 0.007947367908290915, 'activ...\n\n      \n6\n\n      \n2\n\n      \n1.002379\n\n    \n\n    \n\n      \n2-7\n\n      \n{'learning_rate': 0.000491890405781265, 'activ...\n\n      \n7\n\n      \n2\n\n      \n0.280827\n\n    \n\n    \n\n      \n2-8\n\n      \n{'learning_rate': 0.0004272030665166832, 'acti...\n\n      \n8\n\n      \n2\n\n      \n0.124675\n\n    \n\n    \n\n      \n1-0\n\n      \n{'learning_rate': 0.07198931288293847, 'activa...\n\n      \n0\n\n      \n1\n\n      \n0.219331\n\n    \n\n    \n\n      \n1-1\n\n      \n{'learning_rate': 0.003255907153272381, 'activ...\n\n      \n1\n\n      \n1\n\n      \n0.211909\n\n    \n\n    \n\n      \n1-2\n\n      \n{'learning_rate': 0.007926538077878733, 'activ...\n\n      \n2\n\n      \n1\n\n      \n1.420097\n\n    \n\n    \n\n      \n1-3\n\n      \n{'learning_rate': 0.000888230245206736, 'activ...\n\n      \n3\n\n      \n1\n\n      \n0.126166\n\n    \n\n    \n\n      \n1-4\n\n      \n{'learning_rate': 0.0011125712920494182, 'acti...\n\n      \n4\n\n      \n1\n\n      \n0.077499\n\n    \n\n    \n\n      \n0-0\n\n      \n{'learning_rate': 0.018609130883389842, 'activ...\n\n      \n0\n\n      \n0\n\n      \n0.148289\n\n    \n\n    \n\n      \n0-1\n\n      \n{'learning_rate': 0.003873098175478602, 'activ...\n\n      \n1\n\n      \n0\n\n      \n0.098997\n\n    \n\n    \n\n      \n0-2\n\n      \n{'learning_rate': 0.0028061040541777402, 'acti...\n\n      \n2\n\n      \n0\n\n      \n0.410078\n\n    \n\n  \n\n\n\n\n\n\n\nUsing a Generator\n\n\nLet's say you want to stream data using a generator. In that case you need to pass the function that returns a generator (not the generator object itself) to \ngenerator_function\n. You can pass a tuple if you need two different generator functions for training and testing. If they are just different by their arguments you can pass the relevant arguments via \ntrain_gen_args\n and \nvalid_gen_args\n which accept either a dictionary or list/tuple. Hobbit also needs the number of steps / batches in training / testing. You pass these via \nsteps_per_epoch\n and \nvalidation_steps\n similarly as in Keras.\n\n\ndef example_generator(x, y, batch_size=100):\n    num_samples = y.shape[0]\n    num_batches = np.ceil(num_samples/batch_size).astype('int')\n    while True:\n        for i in range(num_batches):\n            from_ = i*batch_size\n            to_ = min((i+1)*batch_size, num_samples)\n            yield x[from_:to_], y[from_:to_]\n\n\n\n\nThen set up Hyperband:\n\n\nhband = Hyperband(model_function=my_model,\n                  hparam_ranges=my_hparam_ranges,\n                  repo_dir='./my_test_repo',\n                  generator_function=example_generator,\n                  train_gen_args=(x_train, y_train, 100),\n                  valid_gen_args=(x_test, y_test, 100),\n                  steps_per_epoch=x_train.shape[0]//100,\n                  validation_steps=x_test.shape[0]//100)",
            "title": "First steps"
        },
        {
            "location": "/first_steps/#first-steps",
            "text": "In this tutorial we will write the code to train a shallow neural network to predict handwritten digits from the MNIST dataset using Keras and optimize it's hyperparameters using Hobbit.",
            "title": "First Steps"
        },
        {
            "location": "/first_steps/#the-hyperparameters",
            "text": "First we need to define distributions and ranges for the hyperparameters we want to include in our model training. To do this we define a list of Hyperparameter objects, each one of these will be tuned afterwards. For each Hyperparameter we must assign a  name , a tuple with the range of values to explore ( args ) and a  distribution  which indicates how to sample values within the defined range.  In this example we want to optimize three hyperparameters: the dropout rate, the activation function and the learning rate. We use uniform(0, 1) for the dropout rate, a discrete distribution over  ('sigmoid', 'tanh', 'relu')  for the activation and a log-uniform for the learning rate. The log-uniform distribution samples uniformly on the log scale and then transforms back. You can see more information about the available distributions  here .  from hobbit import Hyperparameter\nmy_hparam_ranges = [Hyperparameter(name='learning_rate', distribution='log-uniform', distr_args=(0.0001, 0.1)),\n                    Hyperparameter(name='activation', distribution='choice', distr_args=[('sigmoid', 'tanh', 'relu')]),\n                    Hyperparameter(name='dropout', distribution='uniform', distr_args=(0., 1.))]",
            "title": "The Hyperparameters"
        },
        {
            "location": "/first_steps/#the-model",
            "text": "With the principle of keeping Hobbit flexible we completely define the model we want to train inside a function which we call  my_model() . This function takes a  dictionary of hyperparameters   hparams  as its only argument and returns a  compiled Keras model . Every model we train will follow this template and they will differ only in the values of the hyperparameters and how long they were trained.  Every optimizable hyperparameter within  my_model()  corresponds to one of the values defined before as a Hyperparameter object. Here we will specify where each hyperparameter goes and we will access it from the  hparams  dictionary like this  hparams['name_of_hyperparam']  import keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import RMSprop\n\ndef my_model(hparams):\n    model = Sequential()\n    model.add(Dropout(rate=hparams['dropout'], input_shape=(784,)))\n    model.add(Dense(100, activation=hparams['activation']))\n    model.add(Dropout(rate=hparams['dropout']))\n    model.add(Dense(10, activation='softmax'))\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=RMSprop(lr=hparams['learning_rate']),\n                  metrics=['accuracy'])\n    return model",
            "title": "The model"
        },
        {
            "location": "/first_steps/#the-dataset",
            "text": "The dataset is loaded as a tuple of training and validation objects. These objects are usually numpy arrays with each sample as a row and each column as a feature. If there is some preprocessing for the data it has to be done before passing it to Hobbit. Here we load the MNIST dataset using Keras and do some simple preprocessing.  num_classes = 10\n\n# the data, shuffled and split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nx_train = x_train.reshape(60000, 784)\nx_test = x_test.reshape(10000, 784)\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.np_utils.to_categorical(y_test, num_classes)\n\nmy_dataset = (x_train, y_train), (x_test, y_test)",
            "title": "The dataset"
        },
        {
            "location": "/first_steps/#hyperband",
            "text": "We have all the ingredients. Now we just need to create the algorithm. As an argument you need to specify a repository directory. This is where Hobbit will store all of your models and the table with the results.  from hobbit.algorithms import Hyperband\nhband = Hyperband(model_function=my_model,\n                  dataset=my_dataset,\n                  hparam_ranges=my_hparam_ranges,\n                  repo_dir='./my_test_repo')",
            "title": "Hyperband"
        },
        {
            "location": "/first_steps/#running",
            "text": "Finally, let's run the complete pipeline. Hyperband has two parameters:   R : The budget of epochs per stage   eta : The cut-factor after each stage and which is also the factor by which training gets longer at every stage. For eta the theory-default is 3  To give you a feel for these here is an example for R=20 and eta=3. Here n_i is the number of configurations and r_i the number of epochs they are trained for. Hyperband makes multiple runs in which it does successive halving. smax corresponds to the current run.  R = 20\neta = 3\n\nsmax=2\nn_0=9   r_0=2.22=2\nn_1=3   r_1=6.66=7\nn_2=1   r_2=20\n\nsmax=1\nn_0=5   r_0=6.66=7\nn_1=1   r_1=20\n\nsmax=0\nn_0=3   r_0=20  Now let's run our own Hyperband. If you're on a CPU this may take a few minutes.  tab = hband.run(R=20, eta=3)",
            "title": "Running"
        },
        {
            "location": "/first_steps/#results",
            "text": "To access the results for all configurations and their respective test error you can either use the returned Pandas dataframe from the  run()  function or look at the CSV in your repository directory. Val Loss indicates the lowest seen validation loss for the configuration. The  Run  corresponds to the specific run it was trained at according to the Hyperband algorithm with run zero being the last and longest one. The  ID  is an identifier of the model within the run.  tab   \n   \n     \n       \n       Hparams \n       ID \n       Run \n       Val Loss \n     \n   \n   \n     \n       2-0 \n       {'learning_rate': 0.0013383297653888713, 'acti... \n       0 \n       2 \n       0.079573 \n     \n     \n       2-1 \n       {'learning_rate': 0.00012199066883461425, 'act... \n       1 \n       2 \n       0.504462 \n     \n     \n       2-2 \n       {'learning_rate': 0.00012022855306991257, 'act... \n       2 \n       2 \n       0.361248 \n     \n     \n       2-3 \n       {'learning_rate': 0.01711893606634988, 'activa... \n       3 \n       2 \n       0.110820 \n     \n     \n       2-4 \n       {'learning_rate': 0.0002600277942901727, 'acti... \n       4 \n       2 \n       0.442517 \n     \n     \n       2-5 \n       {'learning_rate': 0.0004934370333196648, 'acti... \n       5 \n       2 \n       0.287478 \n     \n     \n       2-6 \n       {'learning_rate': 0.007947367908290915, 'activ... \n       6 \n       2 \n       1.002379 \n     \n     \n       2-7 \n       {'learning_rate': 0.000491890405781265, 'activ... \n       7 \n       2 \n       0.280827 \n     \n     \n       2-8 \n       {'learning_rate': 0.0004272030665166832, 'acti... \n       8 \n       2 \n       0.124675 \n     \n     \n       1-0 \n       {'learning_rate': 0.07198931288293847, 'activa... \n       0 \n       1 \n       0.219331 \n     \n     \n       1-1 \n       {'learning_rate': 0.003255907153272381, 'activ... \n       1 \n       1 \n       0.211909 \n     \n     \n       1-2 \n       {'learning_rate': 0.007926538077878733, 'activ... \n       2 \n       1 \n       1.420097 \n     \n     \n       1-3 \n       {'learning_rate': 0.000888230245206736, 'activ... \n       3 \n       1 \n       0.126166 \n     \n     \n       1-4 \n       {'learning_rate': 0.0011125712920494182, 'acti... \n       4 \n       1 \n       0.077499 \n     \n     \n       0-0 \n       {'learning_rate': 0.018609130883389842, 'activ... \n       0 \n       0 \n       0.148289 \n     \n     \n       0-1 \n       {'learning_rate': 0.003873098175478602, 'activ... \n       1 \n       0 \n       0.098997 \n     \n     \n       0-2 \n       {'learning_rate': 0.0028061040541777402, 'acti... \n       2 \n       0 \n       0.410078",
            "title": "Results"
        },
        {
            "location": "/first_steps/#using-a-generator",
            "text": "Let's say you want to stream data using a generator. In that case you need to pass the function that returns a generator (not the generator object itself) to  generator_function . You can pass a tuple if you need two different generator functions for training and testing. If they are just different by their arguments you can pass the relevant arguments via  train_gen_args  and  valid_gen_args  which accept either a dictionary or list/tuple. Hobbit also needs the number of steps / batches in training / testing. You pass these via  steps_per_epoch  and  validation_steps  similarly as in Keras.  def example_generator(x, y, batch_size=100):\n    num_samples = y.shape[0]\n    num_batches = np.ceil(num_samples/batch_size).astype('int')\n    while True:\n        for i in range(num_batches):\n            from_ = i*batch_size\n            to_ = min((i+1)*batch_size, num_samples)\n            yield x[from_:to_], y[from_:to_]  Then set up Hyperband:  hband = Hyperband(model_function=my_model,\n                  hparam_ranges=my_hparam_ranges,\n                  repo_dir='./my_test_repo',\n                  generator_function=example_generator,\n                  train_gen_args=(x_train, y_train, 100),\n                  valid_gen_args=(x_test, y_test, 100),\n                  steps_per_epoch=x_train.shape[0]//100,\n                  validation_steps=x_test.shape[0]//100)",
            "title": "Using a Generator"
        },
        {
            "location": "/hyperband/",
            "text": "A Hyperband instance initializes the entire pipeline needed to run a\nHyperband hyperparameter optimization. The run() method is used to start\nthe optimization.\n\n\nArguments\n\n\n\n\nmodel_function\n: a function that takes a dictionary of hyperparameters\n    as its only argument and returns a compiled Keras model object with\n    those hyperparameters\n\n\nhparam_ranges\n: a list of Hyperparameter objects\n\n\nrepo_dir\n: the directory to store weights and results table in\n\n\ndataset\n: a dataset of the form ((x_train, y_train), (x_valid, y_valid))\n    where x_, y_ are NumPy arrays\n\n\ngenerator_function\n: alternatively to dataset, a generator function can\n    be passed or a tuple of generator functions. This is a function\n    that returns a generator, not a generator itself. For a tuple\n    the first item is the generator function for training, the second\n    for validation.\n\n\ntrain_gen_args\n: arguments to be passed to generator_function when\n    producing a training generator\n\n\nsteps_per_epoch\n: number of batches for one epoch of training when\n    using a generator\n\n\nvalid_gen_args\n: arguments to be passed to generator_function when\n    producing a validation generator\n\n\nvalidation_steps\n: number of batches for one epoch of validation when\n    using a generator\n\n\n\n\nMethods\n\n\nRuns the algorithm with \nR\n maximum epochs per stage and cut factor\n\neta\n between stages.\n\n\nrun\n\n\n\n\nR\n: The maximum epochs per stage. Hyperband has multiple runs each of\n    which goes through multiple stages to discard configurations. At each\n    of those stages Hyperband will train for a total of R epochs\n\n\neta\n: The cut-factor. After each stage Hyperband will reduce the number\n    of configurations by this factor. The training\n    iterations for configurations that move to the next stage increase\n    by this factor",
            "title": "Hyperband"
        },
        {
            "location": "/hyperparameter/",
            "text": "A Hyperparameter instance captures the information about each of the hyperparameters to be optimized.\n\n\nArguments\n\n\n\n\nname\n: name of the hyperparameter. This will be used in the Model creation\n\n\ndistr_args\n: List, Tuple or Dictionary, it used by the distribution function as arguments. \n        In the default case of a Uniform distribution these refer to the minimum and maximum values from \n        which to sample from. In general these are the  arguments taken as input by the corresponding numpy \n        distribution function.\n\n\ndistribution\n: String, name of the distribution to be used for sampling the values. Must be numpy.random compatible. \n          Uniform distribution is used as default.\n\n\n\n\nExamples\n\n\nHyperparameter('learning_rate', distr_args=(0.0001, 0.1), distribution='log-uniform'),\nHyperparameter('learning_rate', distr_args={low: 0.0001, high: 0.1}, distribution='uniform'),\nHyperparameter('activation', distr_args=[('sigmoid', 'tanh', 'relu')], distribution='choice')",
            "title": "Hyperparameters"
        }
    ]
}